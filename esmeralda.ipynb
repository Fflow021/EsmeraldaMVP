{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14912568,"datasetId":9542000,"databundleVersionId":15778409}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ‚ùáÔ∏è Esmeralda: Preceptoria H√≠brida e IA Socr√°tica\n### O resgate da profici√™ncia m√©dica no Brasil\n\n**Equipe:** Eduardo Chuairi (Produto), Marllon Pereira (IA), Phillipe Wolff (Back-end) e Jayme Ricardo (Front-end).\n\nEste notebook documenta a arquitetura t√©cnica e a prova de conceito (PoC) da Esmeralda. A aplica√ß√£o utiliza o modelo **MedGemma 1.5 4B Multimodal** (HAI-DEF) acoplado a uma rigorosa Engenharia de Prompt para atuar como uma preceptora s√™nior. Em vez de fornecer diagn√≥sticos prontos, o sistema utiliza o M√©todo Socr√°tico para for√ßar o racioc√≠nio cl√≠nico de m√©dicos em forma√ß√£o.","metadata":{}},{"cell_type":"markdown","source":"# Installs\nInstala√ß√£o e atualiza√ß√£o das bibliotecas essenciais para a arquitetura do sistema, abrangendo orquestra√ß√£o RAG, vetoriza√ß√£o de dados, infer√™ncia em GPU e estrutura√ß√£o do servidor de API.","metadata":{}},{"cell_type":"markdown","source":"**1. Orquestra√ß√£o e Ingest√£o de Dados (RAG):**\n* `langchain`, `langchain-core`, `langchain-community`: Estrutura base para orquestrar o fluxo de dados entre a IA e os documentos externos.\n* `langchain-text-splitters`: Segmenta os protocolos m√©dicos extensos em blocos menores (chunks) para otimizar o processamento.\n* `pypdf`: Motor de extra√ß√£o de texto para a leitura nativa dos protocolos em PDF.\n\n**2. Vetoriza√ß√£o e Busca Sem√¢ntica:**\n* `chromadb`: Banco de dados vetorial em mem√≥ria, respons√°vel por armazenar o conhecimento cl√≠nico do SUS.\n* `sentence-transformers`, `langchain-huggingface`: Motores que convertem os textos m√©dicos em vetores matem√°ticos (embeddings) para a busca de similaridade.","metadata":{}},{"cell_type":"code","source":"!pip install -U -q \\\n    langchain \\\n    langchain-community \\\n    langchain-core \\\n    langchain-huggingface \\\n    langchain-text-splitters \\\n    chromadb \\\n    sentence-transformers \\\n    transformers \\\n    accelerate \\\n    bitsandbytes \\\n    pypdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Infer√™ncia e Otimiza√ß√£o de GPU (Edge AI):**\n* `torch`: Framework base de Deep Learning respons√°vel por gerenciar os tensores na placa de v√≠deo.\n* `transformers`, `huggingface_hub`: Gerenciam o download dos pesos e o pipeline de execu√ß√£o do modelo MedGemma 1.5.\n* `accelerate`, `bitsandbytes`: Otimizam o uso da VRAM, viabilizando a execu√ß√£o local do modelo em hardware de consumo (Edge AI).\n* `pillow`: Biblioteca de vis√£o computacional para decodificar e processar imagens de exames m√©dicos.\n\n**4. Servidor de API RESTful:**\n* `fastapi`, `uvicorn`: Estruturam o servidor ass√≠ncrono de alto desempenho que receber√° as requisi√ß√µes do front-end.\n* `requests`: Gerencia as requisi√ß√µes HTTP.\n* `pyngrok`: Cria o t√∫nel de rede que exp√µe temporariamente a API local para acesso externo.\n* `nest-asyncio`: Modifica a gest√£o de eventos do ambiente interativo (Kaggle/Jupyter) para permitir a execu√ß√£o cont√≠nua do servidor ass√≠ncrono.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers pillow torch requests huggingface_hub accelerate fastapi uvicorn pyngrok nest-asyncio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Login HF \n### Autentica√ß√£o Segura\nAutentica√ß√£o segura na plataforma Hugging Face. O c√≥digo utiliza o `UserSecretsClient` para resgatar o token de acesso (`HF_TOKEN`) configurado nas vari√°veis de ambiente restritas do Kaggle. Essa abordagem protege a credencial contra vazamentos no c√≥digo-fonte, sendo uma etapa obrigat√≥ria para o download dos pesos do modelo MedGemma. O bloco inclui um tratamento de exce√ß√µes (try/except) para alertar sobre falhas na leitura da chave.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"‚úÖ Login no Hugging Face realizado com sucesso!\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è N√£o foi poss√≠vel fazer login autom√°tico. Verifique se a Secret 'HF_TOKEN' est√° configurada.\")\n    # Se falhar, voc√™ pode descomentar a linha abaixo para logar manualmente:\n    # login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG\n### Base de Conhecimento (RAG)\n\nConstru√ß√£o da base de conhecimento cl√≠nica do sistema utilizando Retrieval-Augmented Generation (RAG). O script executa o pipeline de ingest√£o e vetoriza√ß√£o em quatro etapas fundamentais:\n\n* **Ingest√£o de Dados (`PyPDFLoader`):** Busca dinamicamente os protocolos oficiais do SUS em formato PDF no diret√≥rio do Kaggle e extrai o conte√∫do textual, ignorando falhas de leitura.\n* **Segmenta√ß√£o (`TextSplitter`):** Divide os documentos extensos em fragmentos menores (*chunks* de 800 caracteres com sobreposi√ß√£o de 100), garantindo que a informa√ß√£o caiba na janela de contexto do LLM sem perda de continuidade.\n* **Vetoriza√ß√£o (`HuggingFaceEmbeddings`):** Utiliza um modelo *sentence-transformer* multil√≠ngue otimizado para gerar representa√ß√µes sem√¢nticas densas (embeddings) dos trechos m√©dicos.\n* **Armazenamento e Recupera√ß√£o (`Chroma`):** Inicializa um banco de dados vetorial em mem√≥ria contendo as diretrizes vetorizadas. O sistema configura um *retriever* para resgatar os 3 fragmentos matematicamente mais relevantes a cada consulta do m√©dico.","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n# --- CONFIGURA√á√ÉO DOS ARQUIVOS ---\ntarget_files = [\n    \"Manejo_clinico_da_Dengue.pdf\",\n    \"Manejo-dengue.pdf\",\n    \"Manejo_clinico_da_Dengue.pdf\",\n    \"dengue_aspecto_epidemiologicos_diagnostico_tratamento.pdf\"\n]\n\ndef find_file_path(filename, search_path='/kaggle'):\n    \"\"\"Procura o arquivo em todo o diret√≥rio do Kaggle\"\"\"\n    for root, dirs, files in os.walk(search_path):\n        if filename in files:\n            return os.path.join(root, filename)\n    return None\n\n# --- CARREGAMENTO ---\ndocs = []\nprint(\"--- üîç Buscando arquivos ---\")\n\nfor file_name in target_files:\n    full_path = find_file_path(file_name)\n    if full_path:\n        print(f\"üìÑ Carregando: {file_name}\")\n        try:\n            loader = PyPDFLoader(full_path)\n            docs.extend(loader.load())\n        except Exception as e:\n            print(f\"‚ùå Erro em {file_name}: {e}\")\n    else:\n        print(f\"‚ö†Ô∏è Arquivo n√£o encontrado: {file_name}\")\n\nif not docs:\n    raise ValueError(\"Nenhum documento carregado! Fa√ßa o upload dos PDFs no Kaggle.\")\n\n# --- DIVIS√ÉO (SPLITTING) ---\n# Chunks menores funcionam melhor para inserir no contexto do LLM\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\nsplits = text_splitter.split_documents(docs)\nprint(f\"üìö Total de trechos processados: {len(splits)}\")\n\n# --- EMBEDDINGS & VECTOR STORE ---\nprint(\"üß† Gerando banco vetorial (aguarde)...\")\nembedding_model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n\n# Cria o banco em mem√≥ria\nvectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Pega os 3 trechos mais relevantes\n\nprint(\"‚úÖ Sistema RAG pronto para uso!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pipe\n### Instancia√ß√£o do Modelo Multimodal\n\nCarregamento do modelo fundacional e orquestra√ß√£o do Motor Socr√°tico. Este bloco consolida o \"c√©rebro\" da aplica√ß√£o e define as regras estritas de comportamento da IA atrav√©s das seguintes etapas:\n\n* **Instancia√ß√£o do Modelo (`pipeline`):** Carrega os pesos do MedGemma 1.5 4B na mem√≥ria da GPU. A utiliza√ß√£o da precis√£o `bfloat16` otimiza severamente o consumo de VRAM, permitindo infer√™ncia local r√°pida (Edge AI).\n* **Conting√™ncia Multimodal (`dummy_image`):** Implementa um *fallback* gerando uma matriz visual vazia (imagem preta). Isso satisfaz o requisito de entrada do modelo de vis√£o-linguagem (VLM) para cen√°rios onde o m√©dico envia apenas texto, sem exames de imagem anexados.\n* **Orquestra√ß√£o de Contexto (Retrieval):** A fun√ß√£o `perguntar_ao_medgemma` aciona o *retriever* configurado no passo anterior para buscar os protocolos do SUS no banco vetorial que respondam √† d√∫vida do usu√°rio.\n* **Engenharia de Prompt (Motor Socr√°tico):** Constr√≥i um prompt restritivo que injeta o contexto do RAG e for√ßa a IA a atuar como a preceptora s√™nior \"Esmeralda\". O modelo √© travado por regras r√≠gidas que o pro√≠bem de fornecer diagn√≥sticos ou dosagens diretas, obrigando-o a responder apenas com perguntas reflexivas adaptadas √† infraestrutura local do m√©dico (M√©todo Socr√°tico).\n* **Infer√™ncia:** Monta o vetor multimodal (imagem + texto) e executa a gera√ß√£o da resposta na GPU limitando os tokens de sa√≠da.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nfrom PIL import Image\n\nprint(\"--- üè• Carregando MedGemma 1.5 ---\")\n\n# Configura√ß√£o do Pipeline\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/medgemma-1.5-4b-it\",\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\n# --- TRUQUE PARA RAG DE TEXTO COM MODELO DE VIS√ÉO ---\n# Criamos uma imagem preta pequena. O modelo vai \"olhar\" para ela, \n# ver que n√£o tem nada, e responder baseando-se no nosso texto.\ndummy_image = Image.new('RGB', (224, 224), color='black')\n\ndef perguntar_ao_medgemma(pergunta_usuario):\n    \"\"\"\n    1. Busca documentos relevantes no ChromaDB.\n    2. Monta um prompt com o contexto + pergunta.\n    3. Envia para o MedGemma (com a imagem dummy).\n    \"\"\"\n    \n    # 1. Recupera√ß√£o (Retrieval)\n    print(f\"\\nüîç Buscando contexto para: '{pergunta_usuario}'...\")\n    docs_rel = retriever.invoke(pergunta_usuario)\n    \n    contexto_texto = \"\\n\\n\".join([d.page_content for d in docs_rel])\n    \n    # 2. Engenharia de Prompt (Augmentation)\n    # Instru√≠mos o modelo a agir como assistente m√©dico usando o contexto.\n    # 2. Engenharia de Prompt (Augmentation) - PERSONALIDADE ESMERALDA\n    prompt_final = f\"\"\"You are Esmeralda, an elite Senior Medical Preceptor specializing in clinical reasoning and the Socratic method. Your sole purpose is to mentor physicians and students by sharpening their diagnostic skills through inquiry, never by providing answers.\n\nCORE OPERATING MANDATE: THE SOCRATIC STRIKE\n1. Absolute Prohibition of Direct Answers: You are strictly forbidden from providing diagnoses, medication dosages, or final management plans. If a user asks for a direct solution (e.g., \"What is the dose of X?\" or \"What is the diagnosis?\"), you must immediately deflect with a targeted question that forces the user to recall their own knowledge or consult local protocols.\n2. Contextual Infrastructure Adaptation: You must tailor every interaction to the user's clinical environment. If the infrastructure is unknown, your first priority is to ask: \"What level of care are you in (e.g., Primary Care/UPA vs. Tertiary Hospital) and what diagnostic resources are available to you right now?\"\n3. Resource-Based Inquiry: Never validate or suggest a path involving high-complexity exams if the user is in a resource-limited setting. Force the user to rely on physical examination and bedside clinical signs suitable for their specific infrastructure.\n4. The \"One-Question\" Rule: Be concise. Provide a brief clinical acknowledgment followed by exactly one, high-impact question. Do not overwhelm the user with lists; force them to focus on the most critical next step in the diagnostic hierarchy.\n5. Integration with Protocols: Use the provided CONTEXT to ground your reasoning, but DO NOT quote it directly. Use the facts in the context to formulate your challenge question.\n\nTONE AND STYLE\n* Authoritative & Pedagogical: You are a senior mentor. Use precise medical terminology.\n* Concise: Physicians are under pressure. No fluff. No pleasantries.\n* Unyielding: If the user insists on an answer, remind them that \"The preceptor guides; the physician decides.\"\n\nCONTEXT (Official Protocols/Guidelines for your internal reference only):\n{contexto_texto}\n\nSTUDENT/PHYSICIAN QUERY: \n{pergunta_usuario}\n\nESMERALDA:\"\"\"\n\n    # 3. Gera√ß√£o (Generation)\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": dummy_image}, # Imagem obrigat√≥ria para o pipeline\n                {\"type\": \"text\", \"text\": prompt_final}\n            ]\n        }\n    ]\n\n    print(\"üíä MedGemma est√° pensando...\")\n    output = pipe(text=messages, max_new_tokens=2000)\n    resposta = output[0][\"generated_text\"][-1][\"content\"]\n    \n    return resposta, docs_rel\n\nprint(\"‚úÖ MedGemma carregado e fun√ß√£o de perguntas pronta!\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SERVER\n### API RESTful e Exposi√ß√£o do Servidor\nCria√ß√£o e exposi√ß√£o da API RESTful ass√≠ncrona que atua como ponte entre a interface do usu√°rio (front-end) e o Motor Socr√°tico. O c√≥digo est√° estruturado nas seguintes etapas:\n\n* **Configura√ß√£o da API (`FastAPI` & `Pydantic`):** Inicializa o servidor e define o esquema de dados esperado (`MensagemUsuario`), que aceita o ID do usu√°rio, o texto do caso cl√≠nico e, opcionalmente, uma imagem em formato Base64.\n* **Processamento Multimodal (Decodifica√ß√£o e Fallback):** Intercepta o *payload* do front-end. Se uma imagem for enviada, o sistema limpa o cabe√ßalho e decodifica o Base64 para um objeto `PIL.Image`. Caso seja uma consulta puramente textual, o servidor gera automaticamente a matriz visual vazia (imagem preta) exigida pelo modelo de vis√£o.\n* **Integra√ß√£o e Infer√™ncia:** Monta a estrutura de mensagens exigida pelo MedGemma, encapsulando a imagem (ou o fallback) e o texto do usu√°rio, e aciona o `pipe` instanciado no bloco anterior, limitando a resposta a 500 tokens.\n* **Exposi√ß√£o de Rede (`Ngrok` & `Uvicorn`):** Limpa t√∫neis residuais de execu√ß√µes anteriores, autentica a chave do Ngrok via Kaggle Secrets e cria um t√∫nel seguro. Isso exp√µe a porta 8000 do localhost do Kaggle para a internet p√∫blica, gerando o endpoint que ser√° consumido pelo front-end. O `nest_asyncio` permite que o servidor `uvicorn` rode sem travar o *loop* de eventos do notebook.","metadata":{}},{"cell_type":"code","source":"import nest_asyncio\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nfrom pyngrok import ngrok\nimport asyncio\nfrom PIL import Image\nimport requests # Mantido caso precise no futuro, mas n√£o usaremos mais para a imagem\nfrom io import BytesIO\nfrom typing import Optional\nimport base64\nfrom kaggle_secrets import UserSecretsClient\n\n# --- 1. CONFIGURA√á√ÉO DO SERVIDOR ---\nnest_asyncio.apply()\napp = FastAPI()\nhistorico_conversas = {}\n\n# Passo 1: Ajuste do modelo para 'imagem_url' correspondendo ao Frontend\nclass MensagemUsuario(BaseModel):\n    user_id: str\n    texto: str\n    imagem_url: Optional[str] = None\n\n# Health checker\n@app.get(\"/\")\ndef home():\n    return {\"status\": \"MedGemma API Online\"}\n\n\n@app.post(\"/chat\")\nasync def responder_chat(dados: MensagemUsuario):\n    uid = dados.user_id\n    mensagem_usuario = dados.texto\n    imagem_url = dados.imagem_url # Captura a vari√°vel corretamente\n\n    if uid not in historico_conversas:\n        historico_conversas[uid] = []\n\n    historico_conversas[uid].append(f\"Usu√°rio: {mensagem_usuario}\")\n\n    resposta_ia = \"\"\n    image = None\n\n    try:\n        # --- 2. PREPARA√á√ÉO DA IMAGEM ---\n        if imagem_url:\n            print(\"üì• Decodificando imagem enviada em Base64...\")\n\n            # O frontend manda no formato: \"data:image/jpeg;base64,BASE64_STRING\"\n            # Precisamos dividir a string pela v√≠rgula e pegar s√≥ a segunda parte (o c√≥digo base64)\n            if \",\" in imagem_url:\n                base64_data = imagem_url.split(\",\")[1]\n            else:\n                base64_data = imagem_url # Caso j√° venha limpo sem o cabe√ßalho 'data:'\n\n            # Decodifica o texto em Base64 transformando-o em Bytes\n            image_bytes = base64.b64decode(base64_data)\n\n            # Carrega a imagem a partir dos Bytes na mem√≥ria usando PIL\n            image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n\n        else:\n            # Modo apenas texto: Cria a imagem preta de placeholder para o modelo\n            print(\"üìù Modo apenas texto (gerando imagem placeholder)...\")\n            image = Image.new('RGB', (224, 224), color='black')\n\n        # --- 3. PREPARA√á√ÉO DO PROMPT E GERA√á√ÉO ---\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": image},\n                    {\"type\": \"text\", \"text\": mensagem_usuario}\n                ]\n            }\n        ]\n\n        # --- GERA√á√ÉO (Assumindo que 'pipe' est√° definido e configurado no seu ambiente) ---\n        output = pipe(text=messages, max_new_tokens=500)\n        resposta_ia = output[0][\"generated_text\"][-1][\"content\"]\n\n    except Exception as e:\n        print(f\"‚ùå Erro: {e}\")\n        resposta_ia = f\"Desculpe, ocorreu um erro t√©cnico: {str(e)}\"\n\n    historico_conversas[uid].append(f\"MedGemma: {resposta_ia}\")\n\n    return {\n        \"resposta\": resposta_ia,\n        \"historico\": historico_conversas[uid]\n    }\n\n# --- 4. EXPOSI√á√ÉO VIA NGROK ---\n# IMPORTANTE: Garanta que 'userdata' e 'pipe' est√£o declarados antes no seu c√≥digo/notebook\nuser_secrets = UserSecretsClient()\nNGROK_TOKEN = user_secrets.get_secret(\"NGROK_TOKEN\")\nngrok.set_auth_token(NGROK_TOKEN)\n\n# Limpa t√∫neis antigos\nfor tunnel in ngrok.get_tunnels():\n    ngrok.disconnect(tunnel.public_url)\nngrok.kill()\n\npublic_url = ngrok.connect(8000)\nprint(f\"üåç Endpoint P√∫blico: {public_url}\")\n\nconfig = uvicorn.Config(app, port=8000)\nserver = uvicorn.Server(config)\nawait server.serve()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}