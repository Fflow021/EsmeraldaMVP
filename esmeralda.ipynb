{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14912568,"datasetId":9542000,"databundleVersionId":15778409}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# â‡ï¸ Esmeralda: Preceptoria HÃ­brida e IA SocrÃ¡tica\n### O resgate da proficiÃªncia mÃ©dica no Brasil\n\n**Equipe:** Eduardo Chuairi (Produto), Marllon Pereira (IA), Phillipe Wolff (Back-end) e Jayme Ricardo (Front-end).\n\nEste notebook documenta a arquitetura tÃ©cnica e a prova de conceito (PoC) da Esmeralda. A aplicaÃ§Ã£o utiliza o modelo **MedGemma 1.5 4B Multimodal** (HAI-DEF) acoplado a uma rigorosa Engenharia de Prompt para atuar como uma preceptora sÃªnior. Em vez de fornecer diagnÃ³sticos prontos, o sistema utiliza o MÃ©todo SocrÃ¡tico para forÃ§ar o raciocÃ­nio clÃ­nico de mÃ©dicos em formaÃ§Ã£o.","metadata":{}},{"cell_type":"markdown","source":"# Installs\nInstalaÃ§Ã£o e atualizaÃ§Ã£o das bibliotecas essenciais para a arquitetura do sistema, abrangendo orquestraÃ§Ã£o RAG, vetorizaÃ§Ã£o de dados, inferÃªncia em GPU e estruturaÃ§Ã£o do servidor de API.","metadata":{}},{"cell_type":"markdown","source":"**1. OrquestraÃ§Ã£o e IngestÃ£o de Dados (RAG):**\n* `langchain`, `langchain-core`, `langchain-community`: Estrutura base para orquestrar o fluxo de dados entre a IA e os documentos externos.\n* `langchain-text-splitters`: Segmenta os protocolos mÃ©dicos extensos em blocos menores (chunks) para otimizar o processamento.\n* `pypdf`: Motor de extraÃ§Ã£o de texto para a leitura nativa dos protocolos em PDF.\n\n**2. VetorizaÃ§Ã£o e Busca SemÃ¢ntica:**\n* `chromadb`: Banco de dados vetorial em memÃ³ria, responsÃ¡vel por armazenar o conhecimento clÃ­nico do SUS.\n* `sentence-transformers`, `langchain-huggingface`: Motores que convertem os textos mÃ©dicos em vetores matemÃ¡ticos (embeddings) para a busca de similaridade.","metadata":{}},{"cell_type":"code","source":"!pip install -U -q \\\n    langchain \\\n    langchain-community \\\n    langchain-core \\\n    langchain-huggingface \\\n    langchain-text-splitters \\\n    chromadb \\\n    sentence-transformers \\\n    transformers \\\n    accelerate \\\n    bitsandbytes \\\n    pypdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T19:38:49.781714Z","iopub.execute_input":"2026-02-21T19:38:49.782181Z","iopub.status.idle":"2026-02-21T19:39:36.373264Z","shell.execute_reply.started":"2026-02-21T19:38:49.782132Z","shell.execute_reply":"2026-02-21T19:39:36.372144Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m501.4/501.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m494.2/494.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.0/331.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ngoogle-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**3. InferÃªncia e OtimizaÃ§Ã£o de GPU (Edge AI):**\n* `torch`: Framework base de Deep Learning responsÃ¡vel por gerenciar os tensores na placa de vÃ­deo.\n* `transformers`, `huggingface_hub`: Gerenciam o download dos pesos e o pipeline de execuÃ§Ã£o do modelo MedGemma 1.5.\n* `accelerate`, `bitsandbytes`: Otimizam o uso da VRAM, viabilizando a execuÃ§Ã£o local do modelo em hardware de consumo (Edge AI).\n* `pillow`: Biblioteca de visÃ£o computacional para decodificar e processar imagens de exames mÃ©dicos.\n\n**4. Servidor de API RESTful:**\n* `fastapi`, `uvicorn`: Estruturam o servidor assÃ­ncrono de alto desempenho que receberÃ¡ as requisiÃ§Ãµes do front-end.\n* `requests`: Gerencia as requisiÃ§Ãµes HTTP.\n* `pyngrok`: Cria o tÃºnel de rede que expÃµe temporariamente a API local para acesso externo.\n* `nest-asyncio`: Modifica a gestÃ£o de eventos do ambiente interativo (Kaggle/Jupyter) para permitir a execuÃ§Ã£o contÃ­nua do servidor assÃ­ncrono.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers pillow torch requests huggingface_hub accelerate fastapi uvicorn pyngrok nest-asyncio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T19:39:36.375044Z","iopub.execute_input":"2026-02-21T19:39:36.375440Z","iopub.status.idle":"2026-02-21T19:39:40.955985Z","shell.execute_reply.started":"2026-02-21T19:39:36.375404Z","shell.execute_reply":"2026-02-21T19:39:40.954959Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Login HF \n### AutenticaÃ§Ã£o Segura\nAutenticaÃ§Ã£o segura na plataforma Hugging Face. O cÃ³digo utiliza o `UserSecretsClient` para resgatar o token de acesso (`HF_TOKEN`) configurado nas variÃ¡veis de ambiente restritas do Kaggle. Essa abordagem protege a credencial contra vazamentos no cÃ³digo-fonte, sendo uma etapa obrigatÃ³ria para o download dos pesos do modelo MedGemma. O bloco inclui um tratamento de exceÃ§Ãµes (try/except) para alertar sobre falhas na leitura da chave.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"âœ… Login no Hugging Face realizado com sucesso!\")\nexcept Exception as e:\n    print(\"âš ï¸ NÃ£o foi possÃ­vel fazer login automÃ¡tico. Verifique se a Secret 'HF_TOKEN' estÃ¡ configurada.\")\n    # Se falhar, vocÃª pode descomentar a linha abaixo para logar manualmente:\n    # login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T19:39:40.957497Z","iopub.execute_input":"2026-02-21T19:39:40.957886Z","iopub.status.idle":"2026-02-21T19:39:41.631795Z","shell.execute_reply.started":"2026-02-21T19:39:40.957812Z","shell.execute_reply":"2026-02-21T19:39:41.630730Z"}},"outputs":[{"name":"stdout","text":"âœ… Login no Hugging Face realizado com sucesso!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# RAG\n### Base de Conhecimento (RAG)\n\nConstruÃ§Ã£o da base de conhecimento clÃ­nica do sistema utilizando Retrieval-Augmented Generation (RAG). O script executa o pipeline de ingestÃ£o e vetorizaÃ§Ã£o em quatro etapas fundamentais:\n\n* **IngestÃ£o de Dados (`PyPDFLoader`):** Busca dinamicamente os protocolos oficiais do SUS em formato PDF no diretÃ³rio do Kaggle e extrai o conteÃºdo textual, ignorando falhas de leitura.\n* **SegmentaÃ§Ã£o (`TextSplitter`):** Divide os documentos extensos em fragmentos menores (*chunks* de 800 caracteres com sobreposiÃ§Ã£o de 100), garantindo que a informaÃ§Ã£o caiba na janela de contexto do LLM sem perda de continuidade.\n* **VetorizaÃ§Ã£o (`HuggingFaceEmbeddings`):** Utiliza um modelo *sentence-transformer* multilÃ­ngue otimizado para gerar representaÃ§Ãµes semÃ¢nticas densas (embeddings) dos trechos mÃ©dicos.\n* **Armazenamento e RecuperaÃ§Ã£o (`Chroma`):** Inicializa um banco de dados vetorial em memÃ³ria contendo as diretrizes vetorizadas. O sistema configura um *retriever* para resgatar os 3 fragmentos matematicamente mais relevantes a cada consulta do mÃ©dico.","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n# --- CONFIGURAÃ‡ÃƒO DOS ARQUIVOS ---\ntarget_files = [\n    \"Manejo_clinico_da_Dengue.pdf\",\n    \"Manejo-dengue.pdf\",\n    \"Manejo_clinico_da_Dengue.pdf\",\n    \"dengue_aspecto_epidemiologicos_diagnostico_tratamento.pdf\"\n]\n\ndef find_file_path(filename, search_path='/kaggle'):\n    \"\"\"Procura o arquivo em todo o diretÃ³rio do Kaggle\"\"\"\n    for root, dirs, files in os.walk(search_path):\n        if filename in files:\n            return os.path.join(root, filename)\n    return None\n\n# --- CARREGAMENTO ---\ndocs = []\nprint(\"--- ğŸ” Buscando arquivos ---\")\n\nfor file_name in target_files:\n    full_path = find_file_path(file_name)\n    if full_path:\n        print(f\"ğŸ“„ Carregando: {file_name}\")\n        try:\n            loader = PyPDFLoader(full_path)\n            docs.extend(loader.load())\n        except Exception as e:\n            print(f\"âŒ Erro em {file_name}: {e}\")\n    else:\n        print(f\"âš ï¸ Arquivo nÃ£o encontrado: {file_name}\")\n\nif not docs:\n    raise ValueError(\"Nenhum documento carregado! FaÃ§a o upload dos PDFs no Kaggle.\")\n\n# --- DIVISÃƒO (SPLITTING) ---\n# Chunks menores funcionam melhor para inserir no contexto do LLM\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\nsplits = text_splitter.split_documents(docs)\nprint(f\"ğŸ“š Total de trechos processados: {len(splits)}\")\n\n# --- EMBEDDINGS & VECTOR STORE ---\nprint(\"ğŸ§  Gerando banco vetorial (aguarde)...\")\nembedding_model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n\n# Cria o banco em memÃ³ria\nvectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Pega os 3 trechos mais relevantes\n\nprint(\"âœ… Sistema RAG pronto para uso!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T19:39:41.634042Z","iopub.execute_input":"2026-02-21T19:39:41.634437Z","iopub.status.idle":"2026-02-21T19:41:03.284773Z","shell.execute_reply.started":"2026-02-21T19:39:41.634408Z","shell.execute_reply":"2026-02-21T19:41:03.283923Z"}},"outputs":[{"name":"stderr","text":"2026-02-21 19:39:57.518586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771702797.762219      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771702797.834661      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771702798.419080      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771702798.419141      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771702798.419144      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771702798.419147      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nWARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n","output_type":"stream"},{"name":"stdout","text":"--- ğŸ” Buscando arquivos ---\nğŸ“„ Carregando: Manejo_clinico_da_Dengue.pdf\nğŸ“„ Carregando: Manejo-dengue.pdf\nğŸ“„ Carregando: Manejo_clinico_da_Dengue.pdf\nğŸ“„ Carregando: dengue_aspecto_epidemiologicos_diagnostico_tratamento.pdf\nğŸ“š Total de trechos processados: 441\nğŸ§  Gerando banco vetorial (aguarde)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f11622f06f4f59a828c4ed693c74f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c87d87ccb3d470085b4baae2ff424d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f9141ba31214399919032a19e1f49ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec98f1dbf8442498490a1ae29ed3d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c17fd72a53e48fd9faa444dc8eff501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ff3711ed33437180ad6ac0b0919f53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/526 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84970f4ca020483f92d07519d5e9a2f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abe4611c1f0489e8020c6ccae6879c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd6d9037bd04dffb8895b4eb8eee356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad15e0d850843d196c0a0fc74d8b390"}},"metadata":{}},{"name":"stdout","text":"âœ… Sistema RAG pronto para uso!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Pipe\n### InstanciaÃ§Ã£o do Modelo Multimodal\n\nCarregamento do modelo fundacional e orquestraÃ§Ã£o do Motor SocrÃ¡tico. Este bloco consolida o \"cÃ©rebro\" da aplicaÃ§Ã£o e define as regras estritas de comportamento da IA atravÃ©s das seguintes etapas:\n\n* **InstanciaÃ§Ã£o do Modelo (`pipeline`):** Carrega os pesos do MedGemma 1.5 4B na memÃ³ria da GPU. A utilizaÃ§Ã£o da precisÃ£o `bfloat16` otimiza severamente o consumo de VRAM, permitindo inferÃªncia local rÃ¡pida (Edge AI).\n* **ContingÃªncia Multimodal (`dummy_image`):** Implementa um *fallback* gerando uma matriz visual vazia (imagem preta). Isso satisfaz o requisito de entrada do modelo de visÃ£o-linguagem (VLM) para cenÃ¡rios onde o mÃ©dico envia apenas texto, sem exames de imagem anexados.\n* **OrquestraÃ§Ã£o de Contexto (Retrieval):** A funÃ§Ã£o `perguntar_ao_medgemma` aciona o *retriever* configurado no passo anterior para buscar os protocolos do SUS no banco vetorial que respondam Ã  dÃºvida do usuÃ¡rio.\n* **Engenharia de Prompt (Motor SocrÃ¡tico):** ConstrÃ³i um prompt restritivo que injeta o contexto do RAG e forÃ§a a IA a atuar como a preceptora sÃªnior \"Esmeralda\". O modelo Ã© travado por regras rÃ­gidas que o proÃ­bem de fornecer diagnÃ³sticos ou dosagens diretas, obrigando-o a responder apenas com perguntas reflexivas adaptadas Ã  infraestrutura local do mÃ©dico (MÃ©todo SocrÃ¡tico).\n* **InferÃªncia:** Monta o vetor multimodal (imagem + texto) e executa a geraÃ§Ã£o da resposta na GPU limitando os tokens de saÃ­da.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nfrom PIL import Image\n\nprint(\"--- ğŸ¥ Carregando MedGemma 1.5 ---\")\n\n# ConfiguraÃ§Ã£o do Pipeline\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/medgemma-1.5-4b-it\",\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\n# --- TRUQUE PARA RAG DE TEXTO COM MODELO DE VISÃƒO ---\n# Criamos uma imagem preta pequena. O modelo vai \"olhar\" para ela, \n# ver que nÃ£o tem nada, e responder baseando-se no nosso texto.\ndummy_image = Image.new('RGB', (224, 224), color='black')\n\ndef perguntar_ao_medgemma(pergunta_usuario):\n    \"\"\"\n    1. Busca documentos relevantes no ChromaDB.\n    2. Monta um prompt com o contexto + pergunta.\n    3. Envia para o MedGemma (com a imagem dummy).\n    \"\"\"\n    \n    # 1. RecuperaÃ§Ã£o (Retrieval)\n    print(f\"\\nğŸ” Buscando contexto para: '{pergunta_usuario}'...\")\n    docs_rel = retriever.invoke(pergunta_usuario)\n    \n    contexto_texto = \"\\n\\n\".join([d.page_content for d in docs_rel])\n    \n    # 2. Engenharia de Prompt (Augmentation)\n    # InstruÃ­mos o modelo a agir como assistente mÃ©dico usando o contexto.\n    # 2. Engenharia de Prompt (Augmentation) - PERSONALIDADE ESMERALDA\n    prompt_final = f\"\"\"You are Esmeralda, an elite Senior Medical Preceptor specializing in clinical reasoning and the Socratic method. Your sole purpose is to mentor physicians and students by sharpening their diagnostic skills through inquiry, never by providing answers.\n\nCORE OPERATING MANDATE: THE SOCRATIC STRIKE\n1. Absolute Prohibition of Direct Answers: You are strictly forbidden from providing diagnoses, medication dosages, or final management plans. If a user asks for a direct solution (e.g., \"What is the dose of X?\" or \"What is the diagnosis?\"), you must immediately deflect with a targeted question that forces the user to recall their own knowledge or consult local protocols.\n2. Contextual Infrastructure Adaptation: You must tailor every interaction to the user's clinical environment. If the infrastructure is unknown, your first priority is to ask: \"What level of care are you in (e.g., Primary Care/UPA vs. Tertiary Hospital) and what diagnostic resources are available to you right now?\"\n3. Resource-Based Inquiry: Never validate or suggest a path involving high-complexity exams if the user is in a resource-limited setting. Force the user to rely on physical examination and bedside clinical signs suitable for their specific infrastructure.\n4. The \"One-Question\" Rule: Be concise. Provide a brief clinical acknowledgment followed by exactly one, high-impact question. Do not overwhelm the user with lists; force them to focus on the most critical next step in the diagnostic hierarchy.\n5. Integration with Protocols: Use the provided CONTEXT to ground your reasoning, but DO NOT quote it directly. Use the facts in the context to formulate your challenge question.\n\nTONE AND STYLE\n* Authoritative & Pedagogical: You are a senior mentor. Use precise medical terminology.\n* Concise: Physicians are under pressure. No fluff. No pleasantries.\n* Unyielding: If the user insists on an answer, remind them that \"The preceptor guides; the physician decides.\"\n\nCONTEXT (Official Protocols/Guidelines for your internal reference only):\n{contexto_texto}\n\nSTUDENT/PHYSICIAN QUERY: \n{pergunta_usuario}\n\nESMERALDA:\"\"\"\n\n    # 3. GeraÃ§Ã£o (Generation)\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": dummy_image}, # Imagem obrigatÃ³ria para o pipeline\n                {\"type\": \"text\", \"text\": prompt_final}\n            ]\n        }\n    ]\n\n    print(\"ğŸ’Š MedGemma estÃ¡ pensando...\")\n    output = pipe(text=messages, max_new_tokens=2000)\n    resposta = output[0][\"generated_text\"][-1][\"content\"]\n    \n    return resposta, docs_rel\n\nprint(\"âœ… MedGemma carregado e funÃ§Ã£o de perguntas pronta!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T19:41:03.285730Z","iopub.execute_input":"2026-02-21T19:41:03.286068Z","iopub.status.idle":"2026-02-21T19:42:19.495193Z","shell.execute_reply.started":"2026-02-21T19:41:03.286038Z","shell.execute_reply":"2026-02-21T19:42:19.493713Z"}},"outputs":[{"name":"stdout","text":"--- ğŸ¥ Carregando MedGemma 1.5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f73106e1864059a80edd8fa15d01a8"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7464c63bc640e78c537c181b93cfff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c36a72b54b40c69c9112b71658dfdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe7ac6ad7b14ecab8046a24c2e997a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ea665869f747228f4d0beca6d4c13d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c178d68ca0f4166a0177043a3baf3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd00eed18db24158a897652e2cc50cb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f0cd9aa84af49969d918775fd55c47f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cfc6908755040b6b13677f5b88fd16b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99aa12c6209d48f4b266c10c83beff6f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab503629d5a942f3becf8fdc82ae74d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"909c5711e640429880494f98923a1c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93aa0f240d7241f0b26655fffa0302eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6be5850e5924a0fa981d8b4c3ff12d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2825cf5769848b293f0f57f9920fe66"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"âœ… MedGemma carregado e funÃ§Ã£o de perguntas pronta!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# SERVER\n### API RESTful e ExposiÃ§Ã£o do Servidor\nCriaÃ§Ã£o e exposiÃ§Ã£o da API RESTful assÃ­ncrona que atua como ponte entre a interface do usuÃ¡rio (front-end) e o Motor SocrÃ¡tico. O cÃ³digo estÃ¡ estruturado nas seguintes etapas:\n\n* **ConfiguraÃ§Ã£o da API (`FastAPI` & `Pydantic`):** Inicializa o servidor e define o esquema de dados esperado (`MensagemUsuario`), que aceita o ID do usuÃ¡rio, o texto do caso clÃ­nico e, opcionalmente, uma imagem em formato Base64.\n* **Processamento Multimodal (DecodificaÃ§Ã£o e Fallback):** Intercepta o *payload* do front-end. Se uma imagem for enviada, o sistema limpa o cabeÃ§alho e decodifica o Base64 para um objeto `PIL.Image`. Caso seja uma consulta puramente textual, o servidor gera automaticamente a matriz visual vazia (imagem preta) exigida pelo modelo de visÃ£o.\n* **IntegraÃ§Ã£o e InferÃªncia:** Monta a estrutura de mensagens exigida pelo MedGemma, encapsulando a imagem (ou o fallback) e o texto do usuÃ¡rio, e aciona o `pipe` instanciado no bloco anterior, limitando a resposta a 500 tokens.\n* **ExposiÃ§Ã£o de Rede (`Ngrok` & `Uvicorn`):** Limpa tÃºneis residuais de execuÃ§Ãµes anteriores, autentica a chave do Ngrok via Kaggle Secrets e cria um tÃºnel seguro. Isso expÃµe a porta 8000 do localhost do Kaggle para a internet pÃºblica, gerando o endpoint que serÃ¡ consumido pelo front-end. O `nest_asyncio` permite que o servidor `uvicorn` rode sem travar o *loop* de eventos do notebook.","metadata":{}},{"cell_type":"code","source":"import nest_asyncio\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nfrom pyngrok import ngrok\nimport asyncio\nfrom PIL import Image\nimport requests # Mantido caso precise no futuro, mas nÃ£o usaremos mais para a imagem\nfrom io import BytesIO\nfrom typing import Optional\nimport base64\nfrom kaggle_secrets import UserSecretsClient\n\n# --- 1. CONFIGURAÃ‡ÃƒO DO SERVIDOR ---\nnest_asyncio.apply()\napp = FastAPI()\nhistorico_conversas = {}\n\n# Passo 1: Ajuste do modelo para 'imagem_url' correspondendo ao Frontend\nclass MensagemUsuario(BaseModel):\n    user_id: str\n    texto: str\n    imagem_url: Optional[str] = None\n\n# Health checker\n@app.get(\"/\")\ndef home():\n    return {\"status\": \"MedGemma API Online\"}\n\n\n@app.post(\"/chat\")\nasync def responder_chat(dados: MensagemUsuario):\n    uid = dados.user_id\n    mensagem_usuario = dados.texto\n    imagem_url = dados.imagem_url # Captura a variÃ¡vel corretamente\n\n    if uid not in historico_conversas:\n        historico_conversas[uid] = []\n\n    historico_conversas[uid].append(f\"UsuÃ¡rio: {mensagem_usuario}\")\n\n    resposta_ia = \"\"\n    image = None\n\n    try:\n        # --- 2. PREPARAÃ‡ÃƒO DA IMAGEM ---\n        if imagem_url:\n            print(\"ğŸ“¥ Decodificando imagem enviada em Base64...\")\n\n            # O frontend manda no formato: \"data:image/jpeg;base64,BASE64_STRING\"\n            # Precisamos dividir a string pela vÃ­rgula e pegar sÃ³ a segunda parte (o cÃ³digo base64)\n            if \",\" in imagem_url:\n                base64_data = imagem_url.split(\",\")[1]\n            else:\n                base64_data = imagem_url # Caso jÃ¡ venha limpo sem o cabeÃ§alho 'data:'\n\n            # Decodifica o texto em Base64 transformando-o em Bytes\n            image_bytes = base64.b64decode(base64_data)\n\n            # Carrega a imagem a partir dos Bytes na memÃ³ria usando PIL\n            image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n\n        else:\n            # Modo apenas texto: Cria a imagem preta de placeholder para o modelo\n            print(\"ğŸ“ Modo apenas texto (gerando imagem placeholder)...\")\n            image = Image.new('RGB', (224, 224), color='black')\n\n        # --- 3. PREPARAÃ‡ÃƒO DO PROMPT E GERAÃ‡ÃƒO ---\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": image},\n                    {\"type\": \"text\", \"text\": mensagem_usuario}\n                ]\n            }\n        ]\n\n        # --- GERAÃ‡ÃƒO (Assumindo que 'pipe' estÃ¡ definido e configurado no seu ambiente) ---\n        output = pipe(text=messages, max_new_tokens=500)\n        resposta_ia = output[0][\"generated_text\"][-1][\"content\"]\n\n    except Exception as e:\n        print(f\"âŒ Erro: {e}\")\n        resposta_ia = f\"Desculpe, ocorreu um erro tÃ©cnico: {str(e)}\"\n\n    historico_conversas[uid].append(f\"MedGemma: {resposta_ia}\")\n\n    return {\n        \"resposta\": resposta_ia,\n        \"historico\": historico_conversas[uid]\n    }\n\n# --- 4. EXPOSIÃ‡ÃƒO VIA NGROK ---\n# IMPORTANTE: Garanta que 'userdata' e 'pipe' estÃ£o declarados antes no seu cÃ³digo/notebook\nuser_secrets = UserSecretsClient()\nNGROK_TOKEN = user_secrets.get_secret(\"NGROK_TOKEN\")\nngrok.set_auth_token(NGROK_TOKEN)\n\n# Limpa tÃºneis antigos\nfor tunnel in ngrok.get_tunnels():\n    ngrok.disconnect(tunnel.public_url)\nngrok.kill()\n\npublic_url = ngrok.connect(8000)\nprint(f\"ğŸŒ Endpoint PÃºblico: {public_url}\")\n\nconfig = uvicorn.Config(app, port=8000)\nserver = uvicorn.Server(config)\nawait server.serve()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T19:42:19.497954Z","iopub.execute_input":"2026-02-21T19:42:19.498435Z","iopub.status.idle":"2026-02-21T19:45:14.253744Z","shell.execute_reply.started":"2026-02-21T19:42:19.498398Z","shell.execute_reply":"2026-02-21T19:45:14.252911Z"}},"outputs":[{"name":"stdout","text":"ğŸŒ Endpoint PÃºblico: NgrokTunnel: \"https://uncapitalistic-gibson-germane.ngrok-free.dev\" -> \"http://localhost:8000\"\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [55]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [55]\n","output_type":"stream"}],"execution_count":6}]}